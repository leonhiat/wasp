{
-- This file is processed by Alex (https://www.haskell.org/alex/) and generates
-- the module `Analyzer.Parser.Lexer`

{-# LANGUAGE NamedFieldPuns #-}

module Wasp.Analyzer.Parser.Lexer
  ( lexer
  ) where

import Wasp.Analyzer.Parser.Monad
import Wasp.Analyzer.Parser.Token (Token (..), TokenType (..))
import Wasp.Analyzer.Parser.ParseError (ParseError (..))
import Control.Monad.State.Lazy (gets)
import Control.Monad.Except (throwError)
import Data.Word (Word8)
import Codec.Binary.UTF8.String (encodeChar)
}

-- Character set aliases
$digit = 0-9
$alpha = [a-zA-Z]
$identstart = [_$alpha]
$ident = [_$alpha$digit]
$any = [.$white]

-- Regular expression aliases
@string = \"([^\\\"]|\\.)*\" -- matches string-literal on a single line, from https://stackoverflow.com/a/9260547/3902376
@double = "-"? $digit+ "." $digit+
@integer = "-"? $digit+
@ident = $identstart $ident* "'"*

-- Tokenization rules (regex -> token)
tokens :-

-- Skips whitespace
<0>       $white+ ;

-- Quoter rules:
-- Uses Alex start codes to lex quoted characters with different rules:
-- - On "{=tag", enter <quoter> start code and make a TLQuote token
-- - While in <quoter>, if "tag=}" is seen
--   - If this closing tag matches the opening, enter <0> and make a TRQuote token
--   - Otherwise, stay in <quoter> and make a TQuoted token
-- - Otherwise, take one character at a time and make a TQuoted token
<0>       "{=" @ident { beginQuoter }
<quoter>  @ident "=}" { lexQuoterEndTag }
<quoter>  $any { createValueToken TQuoted }

-- Simple tokens
<0>       "{" { createConstToken TLCurly }
<0>       "}" { createConstToken TRCurly }
<0>       "," { createConstToken TComma }
<0>       ":" { createConstToken TColon }
<0>       "[" { createConstToken TLSquare }
<0>       "]" { createConstToken TRSquare }
<0>       "import" { createConstToken TImport }
<0>       "from" { createConstToken TFrom }
<0>       "true" { createConstToken TTrue }
<0>       "false" { createConstToken TFalse }

-- Strings, numbers, identifiers
<0>       @string { createValueToken $ \s -> TString $ read s }
<0>       @double { createValueToken $ \s -> TDouble $ read s }
<0>       @integer { createValueToken $ \s -> TInt $ read s }
<0>       @ident { createValueToken $ \s -> TIdentifier s }

{

-- Alex needs the input type to be called "AlexInput"
type AlexInput = ParserInput

-- Convert the ParserState's start code to an int for Alex to use
startCodeToInt :: LexerStartCode -> Int
startCodeToInt DefaultStartCode = 0
startCodeToInt (QuoterStartCode _) = quoter

-- | Required by Alex.
--
--   This function is taken from the Alex basic wrapper.
alexGetByte :: AlexInput -> Maybe (Word8, AlexInput)
alexGetByte (c, (b:bs), s) = Just (b, (c, bs, s))
alexGetByte (_, [], []) = Nothing
alexGetByte (_, [], (c:s)) = case encodeChar c of
                               (b:bs) -> Just (b, (c, bs, s))
                               [] -> Nothing

-- | Required by Alex.
--
--   This function is taken from the Alex basic wrapper.
alexInputPrevChar :: AlexInput -> Char
alexInputPrevChar (c, _, _) = c

-- | Lexes a single token from the input.
--
--   This function is designed for use with the Happy monadic parser that uses threaded/monadic lexer.
--   This means that parser, as it is building an AST, asks for a single token at a time from the lexer, on the go.
--   This is done in "continuation" style -> parser calls lexer while passing it the function ('parseToken') via which
--   lexer gives control back to the parser.
--   In such setup both lexer and parser are operating in the same 'Parser' monad.
--   Check https://www.haskell.org/happy/doc/html/sec-monads.html#sec-lexers for more details.
--
--   This function internally calls `alexScan`, which is a function generated by Alex responsible for doing actual lexing/scanning.
lexer :: (Token -> Parser a) -> Parser a
lexer parseToken = do
  input@(previousChar, _, remainingSource) <- gets parserRemainingInput
  startCodeInt <- gets $ startCodeToInt . parserLexerStartCode
  case alexScan input startCodeInt of
    AlexEOF -> do
      createConstToken TEOF "" >>= parseToken
    AlexError _ -> do
      pos <- gets parserSourcePosition
      throwError $ UnexpectedChar previousChar pos
    AlexSkip input' numCharsSkipped -> do
      updatePosition $ take numCharsSkipped remainingSource
      putInput input'
      lexer parseToken
    AlexToken input' tokenLength action -> do
      -- Token is made before `updatePosition` so that its `tokenPosition` points to
      -- the start of the token's lexeme.
      token <- action $ take tokenLength remainingSource
      updatePosition $ take tokenLength remainingSource
      putInput input'
      parseToken token

-- | Takes a lexeme like "{=json" and sets the quoter start code
beginQuoter :: String -> Parser Token
beginQuoter leftQuoteTag = do
  let tag = drop 2 leftQuoteTag
  setStartCode $ QuoterStartCode tag
  createConstToken (TLQuote tag) leftQuoteTag

-- | Takes a lexeme like "json=}" and either ends a quoter or add quoted text to the quoter
lexQuoterEndTag :: String -> Parser Token
lexQuoterEndTag rightQuoteTag = gets parserLexerStartCode >>= \startCode -> case startCode of
  DefaultStartCode -> error "impossible: lexQuoterEndTag with DefaultStartCode"
  QuoterStartCode startTag | startTag == tag -> do
    setStartCode DefaultStartCode
    createConstToken (TRQuote tag) rightQuoteTag
  _ -> do
    createValueToken TQuoted rightQuoteTag
  where
    tag = take (length rightQuoteTag - 2) rightQuoteTag

-- | Makes an action that creates a token from a constant TokenType.
createConstToken :: TokenType -> (String -> Parser Token)
createConstToken tokType lexeme = do
  position <- gets parserSourcePosition
  return $ Token { tokenType = tokType
                 , tokenPosition = position
                 , tokenLexeme = lexeme
                 }

-- | Makes an action that creates a token using the input lexeme.
createValueToken :: (String -> TokenType) -> (String -> Parser Token)
createValueToken getTokenType lexeme = createConstToken (getTokenType lexeme) lexeme
}
